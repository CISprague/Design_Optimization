@article{Kautz1994,
author = {Kautz, R. L.},
doi = {10.1119/1.17742},
issn = {00029505},
journal = {American Journal of Physics},
number = {1},
pages = {59},
title = {{Chaos at the amusement park: Dynamics of the Tilt-A-Whirl}},
url = {http://link.aip.org/link/?AJP/62/59/1{\&}Agg=doi},
volume = {62},
year = {1994}
}

@article{Matlab,
abstract = {The Mathworks (1994-2009). MATLAB: The language of technical computing Computer software. http://www.mathworks.com/products/matlab/.},
author = {Inc., The MathWorks},
doi = {10.1007/s10766-008-0082-5},
isbn = {0885-7458},
issn = {18783554},
journal = {The MathWorks Inc.},
pmid = {173},
title = {{MATLAB (R2016a)}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:MATLAB+The+Language+of+Technical+Computing{\#}0},
year = {2016}
}

@misc{Helton2003,
abstract = {The following techniques for uncertainty and sensitivity analysis are briefly summarized: Monte Carlo analysis, differential analysis, response surface methodology, Fourier amplitude sensitivity test, Sobol' variance decomposition, and fast probability integration. Desirable features of Monte Carlo analysis in conjunction with Latin hypercube sampling are described in discussions of the following topics: (i) properties of random, stratified and Latin hypercube sampling, (ii) comparisons of random and Latin hypercube sampling, (iii) operations involving Latin hypercube sampling (i.e. correlation control, reweighting of samples to incorporate changed distributions, replicated sampling to test reproducibility of results), (iv) uncertainty analysis (i.e. cumulative distribution functions, complementary cumulative distribution functions, box plots), (v) sensitivity analysis (i.e. scatterplots, regression analysis, correlation analysis, rank transformations, searches for nonrandom patterns), and (vi) analyses involving stochastic (i.e. aleatory) and subjective (i.e. epistemic) uncertainty. Published by Elsevier Science Ltd.},
archivePrefix = {arXiv},
arxivId = {physics/0511236},
author = {Helton, J. C. and Davis, F. J.},
booktitle = {Reliability Engineering and System Safety},
doi = {10.1016/S0951-8320(03)00058-9},
eprint = {0511236},
isbn = {0951-8320},
issn = {09518320},
keywords = {Aleatory uncertainty,Epistemic uncertainty,Latin hypercube sampling,Monte Carlo analysis,Random sampling,Sensitivity analysis,Uncertainty analysis},
number = {1},
pages = {23--69},
pmid = {17707944},
primaryClass = {physics},
title = {{Latin hypercube sampling and the propagation of uncertainty in analyses of complex systems}},
volume = {81},
year = {2003}
}

@book{Rasmussen2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl E. and Williams, Christopher K. I.},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
isbn = {026218253X},
issn = {0129-0657},
keywords = {2006,c,c 2006 massachusetts institute,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
volume = {14},
year = {2004}
}

@article{Kennedy1995,
abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described},
author = {Kennedy, J and Eberhart, R},
doi = {10.1109/ICNN.1995.488968},
isbn = {VO - 4},
issn = {19353812},
journal = {Neural Networks, 1995. Proceedings., IEEE International Conference on},
keywords = {Artificial neural networks,Birds,Educational institutions,Genetic algorithms,Humans,Marine animals,Optimization methods,Particle swarm optimization,Performance evaluation,Testing,artificial intelligence,artificial life,evolution,genetic algorithms,multidimensional search,neural nets,neural network,nonlinear functions,optimization,particle swarm,search problems,simulation,social metaphor},
pages = {1942--1948 vol.4},
pmid = {20371407},
title = {{Particle swarm optimization}},
volume = {4},
year = {1995}
}
